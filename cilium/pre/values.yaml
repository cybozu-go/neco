bgp:
  announce:
    loadbalancerIP: false 
  enabled: false
bgpControlPlane:
  enabled: true
bpf:
  hostLegacyRouting: true
  policyMapMax: 65536
cgroup:
  autoMount:
    enabled: false
  hostRoot: /sys/fs/cgroup
cni:
  chainingMode: "generic-veth"
  customConf: true
datapathMode: "veth"
devices: "eth+ eno1+ eno2+"
enableIPv4Masquerade: false
enableIdentityMark: false
externalIPs:
  enabled: true
extraConfig:
  bpf-ct-timeout-regular-any: 1h0m0s
  bpf-ct-timeout-service-any: 1h0m0s
hostPort:
  enabled: false
hubble:
  relay:
    enabled: true
    tls:
      server:
        enabled: true
    rollOutPods: true
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
  tls:
    auto:
      method: "cronJob"
k8sServiceHost: 127.0.0.1
k8sServicePort: 16443
kubeProxyReplacement: "partial"
labels: "k8s:!controller-uid k8s:!job-name k8s:!node k8s:!batch.kubernetes.io/controller-uid k8s:!batch.kubernetes.io/job-name"
loadBalancer:
  # We can't enable XDP Acceleration because rolling restart Cilium with XDP enabled disrupts in-cluster connectivity
  acceleration: disabled
  algorithm: maglev
  dsrDispatch: geneve
  dsrL4Translate: backend
  mode: dsr
maglev:
  hashSeed: 3HCx6JennjWtot2U
nodePort:
  directRoutingDevice: "e+"
  enabled: true
operator:
  rollOutPods: true
  prometheus:
    enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
policyAuditMode: false
policyEnforcementMode: "default"
pprof:
  enabled: true
  address: "0.0.0.0"
  port: 6060
prometheus:
  enabled: true
  metrics:
    - +cilium_bpf_map_pressure
resources:
  requests:
    cpu: 100m
    memory: 400Mi
rollOutCiliumPods: true
sessionAffinity: true
socketLB:
  enabled: true
  hostNamespaceOnly: true
tunnel: "disabled"
upgradeCompatibility: "1.12"
