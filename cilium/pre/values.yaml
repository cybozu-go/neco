bgp:
  announce:
    loadbalancerIP: true
  enabled: true
bpf:
  hostLegacyRouting: true
  policyMapMax: 65536
cni:
  chainingMode: "generic-veth"
  customConf: true
datapathMode: "veth"
devices: "eth+ eno1+ eno2+"
enableIPv4Masquerade: false
enableIdentityMark: false
externalIPs:
  enabled: true
extraConfig:
  bpf-ct-timeout-regular-any: 1h0m0s
  bpf-ct-timeout-service-any: 1h0m0s
hostPort:
  enabled: false
hubble:
  relay:
    enabled: true
    tls:
      server:
        enabled: true
    rollOutPods: true
    resources:
      requests:
        cpu: 100m
        memory: 200Mi
  tls:
    auto:
      method: "cronJob"
  metrics:
    enabled:
      - "dns"
      - "drop:destinationContext=pod|dns|ip;sourceContext=pod|dns|ip"
      - "tcp"
      - "flow:destinationContext=pod|dns|ip;sourceContext=pod|dns|ip"
      - "icmp"
      - "http"
k8sServiceHost: 127.0.0.1
k8sServicePort: 16443
kubeProxyReplacement: "partial"
loadBalancer:
  # We can't enable XDP Acceleration because rolling restart Cilium with XDP enabled disrupts in-cluster connectivity
  acceleration: disabled
  algorithm: maglev
  mode: dsr
maglev:
  hashSeed: 3HCx6JennjWtot2U
nodePort:
  directRoutingDevice: "e+"
  enabled: true
operator:
  rollOutPods: true
  prometheus:
    enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 200Mi
policyAuditMode: false
policyEnforcementMode: "default"
prometheus:
  enabled: true
resources:
  requests:
    cpu: 100m
    memory: 400Mi
rollOutCiliumPods: true
sessionAffinity: true
socketLB:
  enabled: true
  hostNamespaceOnly: true
tunnel: "disabled"
upgradeCompatibility: "1.11"
